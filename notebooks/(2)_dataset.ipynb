{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399a826b",
   "metadata": {},
   "source": [
    "# Optional\n",
    "\n",
    "**Run the cells below if tokenizer needs to be trained. Trained tokenizer can be cloned from huggingface directly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f1c1c",
   "metadata": {},
   "source": [
    "### The purpose of this file is to create a corpus of english, hindi and kannada data for training a translation model. The saved corspus will be used to train a sentence piece model and then a transformer model for translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc989cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from en_indic_transformer import TranslationDataset, TranslationDataLoader, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79db69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()\n",
    "base_dir = path.absolute().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b3f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = base_dir / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a343c",
   "metadata": {},
   "source": [
    "Get the saved english to hindi data and english to kannada data from the respective csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57441f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hindi_file = load_dir / 'en_hindi.csv'\n",
    "en_kannada_file = load_dir / 'en_kannada.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d6a89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/sameergururajmathad/en-indic-transformer/data/en_hindi.csv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hindi_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86872651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/sameergururajmathad/en-indic-transformer/data/en_kannada.csv')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_kannada_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca10247",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hindi_df = pd.read_csv(en_hindi_file)\n",
    "en_kannada_df = pd.read_csv(en_kannada_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5d8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hindi_source = en_hindi_df[\"english_sentence\"].tolist()\n",
    "en_hindi_target = en_hindi_df[\"hindi_sentence\"].tolist()\n",
    "en_kannada_source = en_kannada_df[\"english_sentence\"].tolist()\n",
    "en_kannada_target = en_kannada_df[\"kannada_sentence\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648ffe2",
   "metadata": {},
   "source": [
    "combine all the data into single list to store the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50ef9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "corpus.extend(en_hindi_source)\n",
    "corpus.extend(en_hindi_target)\n",
    "corpus.extend(en_kannada_source)\n",
    "corpus.extend(en_kannada_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a45f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7667100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aeb676",
   "metadata": {},
   "source": [
    "save the processed data into a text file to be used for training sentence piece model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d7659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_save_dir = base_dir / 'data'\n",
    "tokenizer_save_dir = base_dir / 'tokenizer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d86ed",
   "metadata": {},
   "source": [
    "check if the directory exists, if not create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa17a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not corpus_save_dir.exists():\n",
    "    corpus_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not tokenizer_save_dir.exists():\n",
    "    tokenizer_save_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfe9e4",
   "metadata": {},
   "source": [
    "save the corpus to a text file if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8350b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = corpus_save_dir / 'tokenizer_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "791bac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not save_file.exists():\n",
    "#     with open(save_file , 'w', encoding='utf-8') as file:\n",
    "#         file.write('\\n'.join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not save_file.exists():\n",
    "    with open(save_file , 'w', encoding='utf-8') as file:\n",
    "        for item in corpus:\n",
    "            file.write(f'{item}\\n') # safer approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0e08c",
   "metadata": {},
   "source": [
    "Train the tokeinzer. It requires few parameters like input file, model prefix, vocab size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b7ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50_000\n",
    "model_prefix = tokenizer_save_dir / 'tokenizer' # path to store the tokenizer files and also the name to store 'tokenizer'\n",
    "user_defined_symbols = {'<|endoftext|>', '<|english|>', '<|hindi|>', '<|kannada|>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "127d5ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece on the given data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /Users/sameergururajmathad/en-indic-transformer/data/tokenizer_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: /Users/sameergururajmathad/en-indic-transformer/tokenizer/tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 0\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <|english|>\n",
      "  user_defined_symbols: <|kannada|>\n",
      "  user_defined_symbols: <|endoftext|>\n",
      "  user_defined_symbols: <|hindi|>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: /Users/sameergururajmathad/en-indic-transformer/data/tokenizer_corpus.txt\n",
      "trainer_interface.cc(382) LOG(WARNING) Found too long line (11090 > 4192).\n",
      "trainer_interface.cc(384) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(385) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 4000000 lines\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 5000000 lines\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 6000000 lines\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 7000000 lines\n",
      "trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (7667074), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 7667074 sentences\n",
      "trainer_interface.cc(418) LOG(INFO) Skipped 26 too long sentences.\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <|english|>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <|kannada|>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <|endoftext|>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <|hindi|>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=499141154\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 100% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1401\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 7667074 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=267997810\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 1001401 seed sentencepieces\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 7667074 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=821216 obj=133.161 num_tokens=84750117 num_tokens/piece=103.201\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=552227 obj=113.437 num_tokens=85444810 num_tokens/piece=154.728\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=412268 obj=112.156 num_tokens=86092000 num_tokens/piece=208.825\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=399527 obj=111.911 num_tokens=86393168 num_tokens/piece=216.239\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=299592 obj=111.912 num_tokens=86671147 num_tokens/piece=289.297\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=299407 obj=111.873 num_tokens=86752400 num_tokens/piece=289.747\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=224550 obj=111.984 num_tokens=87097295 num_tokens/piece=387.875\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=224538 obj=111.929 num_tokens=87123277 num_tokens/piece=388.011\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=168396 obj=112.218 num_tokens=87804975 num_tokens/piece=521.42\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=168375 obj=112.099 num_tokens=87818979 num_tokens/piece=521.568\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=126277 obj=112.715 num_tokens=89076489 num_tokens/piece=705.405\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=126264 obj=112.485 num_tokens=89088915 num_tokens/piece=705.577\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=94697 obj=113.687 num_tokens=91226689 num_tokens/piece=963.354\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=94689 obj=113.273 num_tokens=91235910 num_tokens/piece=963.532\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=71016 obj=115.3 num_tokens=94419339 num_tokens/piece=1329.55\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=71013 obj=114.676 num_tokens=94425142 num_tokens/piece=1329.69\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=54999 obj=117.194 num_tokens=97968258 num_tokens/piece=1781.27\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=54996 obj=116.524 num_tokens=97970430 num_tokens/piece=1781.41\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: /Users/sameergururajmathad/en-indic-transformer/tokenizer/tokenizer.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: /Users/sameergururajmathad/en-indic-transformer/tokenizer/tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "Tokenizer.train(corpus_path=str(save_file),\n",
    "                save_path=str(model_prefix),\n",
    "                vocab_size=vocab_size, \n",
    "                user_defined_symbols=user_defined_symbols, \n",
    "                model_type='unigram', \n",
    "                split_by_whitespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52f95342",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(str(tokenizer_save_dir/'tokenizer.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dde0c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_en = \"The quick brown fox jumps over the lazy dog.\"\n",
    "txt_hi = \"मुझे हिन्दी बहुत पसंद है।\"\n",
    "txt_kn = \"ನನಗೆ ಕನ್ನಡ ತುಂಬಾ ಇಷ್ಟ.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d94d1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(src=en_hindi_source, target=en_hindi_target, tokenizer=tokenizer, src_prepend_value='<|english|>', target_prepend_value='<|hindi|>', endoftext='<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2173d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    sources, target_ins, target_outs = [], [], []\n",
    "\n",
    "    for source, target_in, target_out in batch:\n",
    "        sources.append(source)\n",
    "        target_ins.append(target_in)\n",
    "        target_outs.append(target_out)\n",
    "\n",
    "    source_padded = pad_sequence(sources, batch_first=True, padding_value=50256)\n",
    "    target_in_padded = pad_sequence(target_ins, batch_first=True, padding_value=50256)\n",
    "    target_out_padded = pad_sequence(target_outs, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return source_padded, target_in_padded, target_out_padded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "720caa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset=dataset, batch_size=16, shuffle=True,collate_fn=custom_collate_fn)\n",
    "dataloader = TranslationDataLoader(dataset=dataset, batch_size=16, shuffle=True, pad_val=tokenizer.get_piece_id('<|endoftext|>'), ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d73d4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c7df0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c421201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = list(first[0][2])\n",
    "# target_in = list(first[1][2])\n",
    "# target_out = list(first[2][2])\n",
    "\n",
    "source = first[0][2]\n",
    "target_in = first[1][2]\n",
    "target_out = first[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "262370f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    6,  4924,   206, 18895, 12035,   843,   373,     5,     5,     5,\n",
       "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "             5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "             5,     5,     5,     5,     5,     5]),\n",
       " tensor([ 4924,   206, 18895, 12035,   843,   373,     5,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_in, target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d93ac08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|hindi|> लेखकः तारा अली बेग<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'लेखकः तारा अली बेग<|endoftext|>')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(target_in), tokenizer.decode(target_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a28b5",
   "metadata": {},
   "source": [
    "checking if the length of target input and output are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20cc71a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 46)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_in), len(target_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d11d35bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|english|> Author: Tara Ali Baig<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([id for id in source if id != -100])\n",
    "tokenizer.decode(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb6b7b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|hindi|> लेखकः तारा अली बेग<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([id for id in target_in if id != -100])\n",
    "tokenizer.decode(target_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "207e0aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'लेखकः तारा अली बेग<|endoftext|>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([id for id in target_out if id != -100])\n",
    "tokenizer.decode(target_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
