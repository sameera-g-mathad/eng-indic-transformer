{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399a826b",
   "metadata": {},
   "source": [
    "# Optional\n",
    "\n",
    "**Run the cells below if tokenizer needs to be trained. Trained tokenizer can be cloned from huggingface directly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f1c1c",
   "metadata": {},
   "source": [
    "### The purpose of this file is to create a corpus of english, hindi and kannada data for training a translation model. The saved corspus will be used to train a sentence piece model and then a transformer model for translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc989cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from en_indic_transformer import TranslationDataset, TranslationDataLoader, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79db69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()\n",
    "base_dir = path.absolute().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b3f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = base_dir / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a343c",
   "metadata": {},
   "source": [
    "Get the saved english to hindi data and english to kannada data from the respective csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57441f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hindi_file = load_dir / 'en_hindi.csv'\n",
    "en_kannada_file = load_dir / 'en_kannada.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d6a89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/sameergururajmathad/eng-indic-transformer/data/en_hindi.csv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hindi_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86872651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/sameergururajmathad/eng-indic-transformer/data/en_kannada.csv')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_kannada_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca10247",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hindi_df = pd.read_csv(en_hindi_file)\n",
    "en_kannada_df = pd.read_csv(en_kannada_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5d8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hindi_source = en_hindi_df[\"english_sentence\"].tolist()\n",
    "en_hindi_target = en_hindi_df[\"hindi_sentence\"].tolist()\n",
    "en_kannada_source = en_kannada_df[\"english_sentence\"].tolist()\n",
    "en_kannada_target = en_kannada_df[\"kannada_sentence\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648ffe2",
   "metadata": {},
   "source": [
    "combine all the data into single list to store the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50ef9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "corpus.extend(en_hindi_source)\n",
    "corpus.extend(en_hindi_target)\n",
    "corpus.extend(en_kannada_source)\n",
    "corpus.extend(en_kannada_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a45f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7667100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aeb676",
   "metadata": {},
   "source": [
    "save the processed data into a text file to be used for training sentence piece model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d7659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_save_dir = base_dir / 'data'\n",
    "tokenizer_save_dir = base_dir / 'tokenizer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d86ed",
   "metadata": {},
   "source": [
    "check if the directory exists, if not create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa17a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not corpus_save_dir.exists():\n",
    "    corpus_save_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfe9e4",
   "metadata": {},
   "source": [
    "save the corpus to a text file if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8350b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = corpus_save_dir / 'tokenizer_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cba579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not save_file.exists():\n",
    "    with open(save_file , 'w', encoding='utf-8') as file:\n",
    "        for item in corpus:\n",
    "            file.write(f'{item}\\n') # safer approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0e08c",
   "metadata": {},
   "source": [
    "Train the tokeinzer. It requires few parameters like input file, model prefix, vocab size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3b7ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50_000\n",
    "model_prefix = tokenizer_save_dir / 'tokenizer' # path to store the tokenizer files and also the name to store 'tokenizer'\n",
    "user_defined_symbols = {'<|endoftext|>', '<|english|>', '<|hindi|>', '<|kannada|>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "127d5ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer folder exists. Skipping training...\n"
     ]
    }
   ],
   "source": [
    "if not tokenizer_save_dir.exists():\n",
    "    tokenizer_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    Tokenizer.train(corpus_path=str(save_file),\n",
    "                save_path=str(model_prefix),\n",
    "                vocab_size=vocab_size, \n",
    "                user_defined_symbols=user_defined_symbols, \n",
    "                model_type='unigram', \n",
    "                split_by_whitespace=False)\n",
    "else:\n",
    "    print('Tokenizer folder exists. Skipping training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f95342",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(str(tokenizer_save_dir/'tokenizer.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dde0c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_en = \"The quick brown fox jumps over the lazy dog.\"\n",
    "txt_hi = \"मुझे हिन्दी बहुत पसंद है।\"\n",
    "txt_kn = \"ನನಗೆ ಕನ್ನಡ ತುಂಬಾ ಇಷ್ಟ.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d94d1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(src=en_hindi_source, target=en_hindi_target, tokenizer=tokenizer, src_prepend_value='<|english|>', target_prepend_value='<|hindi|>', endoftext='<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2173d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    sources, target_ins, target_outs = [], [], []\n",
    "\n",
    "    for source, target_in, target_out in batch:\n",
    "        sources.append(source)\n",
    "        target_ins.append(target_in)\n",
    "        target_outs.append(target_out)\n",
    "\n",
    "    source_padded = pad_sequence(sources, batch_first=True, padding_value=50256)\n",
    "    target_in_padded = pad_sequence(target_ins, batch_first=True, padding_value=50256)\n",
    "    target_out_padded = pad_sequence(target_outs, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return source_padded, target_in_padded, target_out_padded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "720caa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset=dataset, batch_size=16, shuffle=True,collate_fn=custom_collate_fn)\n",
    "dataloader = TranslationDataLoader(dataset=dataset, batch_size=16, shuffle=True, pad_val=tokenizer.get_piece_id('<|endoftext|>'), ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d73d4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c7df0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c421201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = list(first[0][2])\n",
    "# target_in = list(first[1][2])\n",
    "# target_out = list(first[2][2])\n",
    "\n",
    "source = first[0][2]\n",
    "target_in = first[1][2]\n",
    "target_out = first[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "262370f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    5,  4385,   546,  2085,  9544, 11844, 19916,  7743,  8652,    30,\n",
       "            12,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3]),\n",
       " tensor([ 4385,   546,  2085,  9544, 11844, 19916,  7743,  8652,    30,    12,\n",
       "             3,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_in, target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d93ac08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|hindi|> उनमें से हमारे मानव सांधन की गुणवत्ता सर्वाधिक महत्त्वपूर्ण है।<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'उनमें से हमारे मानव सांधन की गुणवत्ता सर्वाधिक महत्त्वपूर्ण है।<|endoftext|>')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(target_in), tokenizer.decode(target_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a28b5",
   "metadata": {},
   "source": [
    "checking if the length of target input and output are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20cc71a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 34)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_in), len(target_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d11d35bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|english|> The quality of our human resources is amongst the most important of them.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([id for id in source if id != -100])\n",
    "tokenizer.decode(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb6b7b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|hindi|> उनमें से हमारे मानव सांधन की गुणवत्ता सर्वाधिक महत्त्वपूर्ण है।<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([id for id in target_in if id != -100])\n",
    "tokenizer.decode(target_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "207e0aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'उनमें से हमारे मानव सांधन की गुणवत्ता सर्वाधिक महत्त्वपूर्ण है।<|endoftext|>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([id for id in target_out if id != -100])\n",
    "tokenizer.decode(target_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
