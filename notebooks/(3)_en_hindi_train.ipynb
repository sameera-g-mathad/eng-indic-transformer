{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e546e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sameergururajmathad/en-indic-transformer/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from en_indic_transformer import Transformer, Tokenizer, Trainer, TranslationDataLoader, TranslationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1b5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68fbbc",
   "metadata": {},
   "source": [
    "Specify the home_dir for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764305f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/sameergururajmathad/en-indic-transformer')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_dir = Path().absolute().parent\n",
    "home_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdaffa6",
   "metadata": {},
   "source": [
    "Create a various values to use for the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8b453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer('gpt2' {'<|english|>','<|hindi|>', '<|kannada|>' }) # adding kannada for later\n",
    "corpus_save_dir = home_dir / 'tokenizer'\n",
    "tokenizer = Tokenizer(str(corpus_save_dir/'tokenizer.model')) # uses sentence-piece tokenizer\n",
    "src_prepend_value = '<|english|>'\n",
    "target_prepend_value = '<|hindi|>'\n",
    "\n",
    "batch_size = 16\n",
    "random_seed = 42 # for reproducibility\n",
    "device: Literal['cpu', 'cuda'] = 'cuda' if torch.cuda.is_available() else 'cpu' # device for training.\n",
    "\n",
    "# apply random_seed\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "# transformer details\n",
    "context_length = 3000\n",
    "vocab_size = tokenizer.n_vocab # since using gpt2 tokenizer\n",
    "emb_dim = 512\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "num_heads = 16\n",
    "dropout = 0.1\n",
    "bias = False\n",
    "\n",
    "# training details\n",
    "epochs = 10\n",
    "lr = 1e-5 # change.\n",
    "\n",
    "# data\n",
    "train_frac = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "272ac497",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = home_dir / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e255bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hindi_csv = data_dir / 'en_hindi.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2c2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hindi_df = pd.read_csv(en_hindi_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25202c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>However, Paes, who was partnering Australia's ...</td>\n",
       "      <td>आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whosoever desires the reward of the world, wit...</td>\n",
       "      <td>और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The value of insects in the biosphere is enorm...</td>\n",
       "      <td>जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mithali To Anchor Indian Team Against Australi...</td>\n",
       "      <td>आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After the assent of the Honble President on 8t...</td>\n",
       "      <td>8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127700</th>\n",
       "      <td>Examples of art deco construction can be found...</td>\n",
       "      <td>आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127701</th>\n",
       "      <td>and put it in our cheeks.</td>\n",
       "      <td>और अपने गालों में डाल लेते हैं।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127702</th>\n",
       "      <td>As for the other derivatives of sulphur , the ...</td>\n",
       "      <td>जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127703</th>\n",
       "      <td>its complicated functioning is defined thus in...</td>\n",
       "      <td>Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127704</th>\n",
       "      <td>They've just won four government contracts to ...</td>\n",
       "      <td>हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127705 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence  \\\n",
       "0       However, Paes, who was partnering Australia's ...   \n",
       "1       Whosoever desires the reward of the world, wit...   \n",
       "2       The value of insects in the biosphere is enorm...   \n",
       "3       Mithali To Anchor Indian Team Against Australi...   \n",
       "4       After the assent of the Honble President on 8t...   \n",
       "...                                                   ...   \n",
       "127700  Examples of art deco construction can be found...   \n",
       "127701                          and put it in our cheeks.   \n",
       "127702  As for the other derivatives of sulphur , the ...   \n",
       "127703  its complicated functioning is defined thus in...   \n",
       "127704  They've just won four government contracts to ...   \n",
       "\n",
       "                                           hindi_sentence  \n",
       "0       आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...  \n",
       "1       और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...  \n",
       "2       जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...  \n",
       "3         आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को  \n",
       "4       8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...  \n",
       "...                                                   ...  \n",
       "127700  आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...  \n",
       "127701                    और अपने गालों में डाल लेते हैं।  \n",
       "127702  जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...  \n",
       "127703  Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .  \n",
       "127704  हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...  \n",
       "\n",
       "[127705 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hindi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c5364",
   "metadata": {},
   "source": [
    "There are 127705 rows in the dataset. Use train_len rows for training and remaining for validation. I am running on cpu. Will use gpu later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b47d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(train_frac * len(en_hindi_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a11f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = en_hindi_df.iloc[:train_len,:]\n",
    "test_df = en_hindi_df.iloc[train_len: :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b9643a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>However, Paes, who was partnering Australia's ...</td>\n",
       "      <td>आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whosoever desires the reward of the world, wit...</td>\n",
       "      <td>और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The value of insects in the biosphere is enorm...</td>\n",
       "      <td>जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mithali To Anchor Indian Team Against Australi...</td>\n",
       "      <td>आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After the assent of the Honble President on 8t...</td>\n",
       "      <td>8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  However, Paes, who was partnering Australia's ...   \n",
       "1  Whosoever desires the reward of the world, wit...   \n",
       "2  The value of insects in the biosphere is enorm...   \n",
       "3  Mithali To Anchor Indian Team Against Australi...   \n",
       "4  After the assent of the Honble President on 8t...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...  \n",
       "1  और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...  \n",
       "2  जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...  \n",
       "3    आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को  \n",
       "4  8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d13d6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114934</th>\n",
       "      <td>very simply.</td>\n",
       "      <td>सरल इलाज जानते हैं.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114935</th>\n",
       "      <td>Take , for instance , man whose brain confers ...</td>\n",
       "      <td>मनुष्य का ही उदाहरण लीजिए.धरती पर विद्यमान सभी...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114936</th>\n",
       "      <td>so there's holes in the opposite corners, ther...</td>\n",
       "      <td>तो इसमे उल्टे कोनों पर छेद हैं, और यहाँ छोटा स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114937</th>\n",
       "      <td>these particular readings,</td>\n",
       "      <td>इन विशेष अनुवादों को मानित करने का</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114938</th>\n",
       "      <td>“First time I smiled doing a derivative.”</td>\n",
       "      <td>“पहली बार डेरिवेशन का सवाल करते हुई मैं मुस्कर...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence  \\\n",
       "114934                                       very simply.   \n",
       "114935  Take , for instance , man whose brain confers ...   \n",
       "114936  so there's holes in the opposite corners, ther...   \n",
       "114937                         these particular readings,   \n",
       "114938          “First time I smiled doing a derivative.”   \n",
       "\n",
       "                                           hindi_sentence  \n",
       "114934                                सरल इलाज जानते हैं.  \n",
       "114935  मनुष्य का ही उदाहरण लीजिए.धरती पर विद्यमान सभी...  \n",
       "114936  तो इसमे उल्टे कोनों पर छेद हैं, और यहाँ छोटा स...  \n",
       "114937                 इन विशेष अनुवादों को मानित करने का  \n",
       "114938  “पहली बार डेरिवेशन का सवाल करते हुई मैं मुस्कर...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264b6aa",
   "metadata": {},
   "source": [
    "Create lists of source and target sentences for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24b4c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "source_train = train_df['english_sentence'].tolist()\n",
    "target_train = train_df['hindi_sentence'].tolist()\n",
    "\n",
    "# test\n",
    "source_test = test_df['english_sentence'].tolist()\n",
    "target_test = test_df['hindi_sentence'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67791a11",
   "metadata": {},
   "source": [
    "Create training and testing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc142909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "train_dataset = TranslationDataset(src=source_train, target=target_train,tokenizer=tokenizer, src_prepend_value=src_prepend_value, target_prepend_value=target_prepend_value)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = TranslationDataset(src=source_test, target=target_test,tokenizer=tokenizer, src_prepend_value=src_prepend_value, target_prepend_value=target_prepend_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d2239d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_val = tokenizer.get_piece_id('<|endoftext|>')\n",
    "ignore_index = -100\n",
    "# train dataloader\n",
    "train_dataloader = TranslationDataLoader(train_dataset, batch_size=batch_size, shuffle=True, pad_val=pad_val, ignore_index=ignore_index)\n",
    "\n",
    "# test dataloader\n",
    "test_dataloader = TranslationDataLoader(test_dataset, batch_size=batch_size, shuffle=True, pad_val=pad_val, ignore_index=ignore_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc9ad5",
   "metadata": {},
   "source": [
    "set aside a input for inference later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5b3b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(test_dataloader)\n",
    "sample_batch = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5b2ec7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|english|> how to autonomously build<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|hindi|>',\n",
       " '<|hindi|> कैसे स्वायत्त रूप से<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.decode(sample_batch[0][0])\n",
    "target = tokenizer.decode(sample_batch[1][0][:1]) # take the starting token for now.\n",
    "actual_target = tokenizer.decode(sample_batch[1][0]) # take the starting token for now.\n",
    "\n",
    "inputs, target, actual_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360deec",
   "metadata": {},
   "source": [
    "Create the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29758699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (token_embeddings): Embedding(50000, 512)\n",
       "    (pos_embeddings): Embedding(3000, 512)\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (mlp): MLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_embeddings): Embedding(50000, 512)\n",
       "    (pos_embeddings): Embedding(3000, 512)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (mlp): MLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (c_attn): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (norm3): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (final_layer): Linear(in_features=512, out_features=50000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(random_seed) # needed to get same weights for reproducibility\n",
    "model = Transformer(vocab_size=vocab_size, context_length=context_length, emb_dim=emb_dim, enc_layers=enc_layers, dec_layers=dec_layers, num_heads=num_heads,dropout=dropout, bias=bias)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29539c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101922816"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb738f8",
   "metadata": {},
   "source": [
    "Create a optimizer and loss function\n",
    "\n",
    "Using Adam optimizer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e287f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5c2cf",
   "metadata": {},
   "source": [
    "### Create the trainer instance for training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745e5db",
   "metadata": {},
   "source": [
    "##### create a path to save model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b228a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_dir = home_dir / 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0401969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "541754f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The path do not exist yet.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer, tokenizer=tokenizer, save_path= model_checkpoint_dir / 'transformer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "769a3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train(train_dataloader=train_dataloader, test_dataloader=test_dataloader, epochs=epochs, device=device, predict_input=inputs, predict_target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46af1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
