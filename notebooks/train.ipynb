{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e546e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/eng-indic-transformer/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from en_indic_transformer import Transformer, Tokenizer, Trainer, TranslationDataLoader, TranslationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1b5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdaffa6",
   "metadata": {},
   "source": [
    "Create a various values to use for the rest of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603adab",
   "metadata": {},
   "source": [
    "Get the data from the data directory and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8b453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('gpt2', extend_base_encoder={'<|english|>','<|hindi|>', '<|kannada|>' }) # adding kannada for later\n",
    "src_prepend_value = '<|english|>'\n",
    "target_prepend_value = '<|hindi|>'\n",
    "\n",
    "batch_size = 16\n",
    "random_seed = 42 # for reproducibility\n",
    "device: Literal['cpu', 'cuda'] = 'cuda' if torch.cuda.is_available() else 'cpu' # device for training.\n",
    "\n",
    "# apply random_seed\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "# transformer details\n",
    "context_length = 3000\n",
    "vocab_size = tokenizer.n_vocab # since using gpt2 tokenizer\n",
    "emb_dim = 512\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "num_heads = 16\n",
    "dropout = 0.1\n",
    "bias = False\n",
    "\n",
    "# training details\n",
    "epochs = 10\n",
    "lr = 1e-5 # change.\n",
    "\n",
    "# data\n",
    "train_len = 100_000\n",
    "test_len = 20_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764305f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = Path().absolute().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "272ac497",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = home_dir / 'data/eng_hindi.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2c2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25202c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>However, Paes, who was partnering Australia's ...</td>\n",
       "      <td>आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whosoever desires the reward of the world, wit...</td>\n",
       "      <td>और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The value of insects in the biosphere is enorm...</td>\n",
       "      <td>जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mithali To Anchor Indian Team Against Australi...</td>\n",
       "      <td>आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After the assent of the Honble President on 8t...</td>\n",
       "      <td>8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127700</th>\n",
       "      <td>Examples of art deco construction can be found...</td>\n",
       "      <td>आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127701</th>\n",
       "      <td>and put it in our cheeks.</td>\n",
       "      <td>और अपने गालों में डाल लेते हैं।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127702</th>\n",
       "      <td>As for the other derivatives of sulphur , the ...</td>\n",
       "      <td>जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127703</th>\n",
       "      <td>its complicated functioning is defined thus in...</td>\n",
       "      <td>Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127704</th>\n",
       "      <td>They've just won four government contracts to ...</td>\n",
       "      <td>हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127705 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence  \\\n",
       "0       However, Paes, who was partnering Australia's ...   \n",
       "1       Whosoever desires the reward of the world, wit...   \n",
       "2       The value of insects in the biosphere is enorm...   \n",
       "3       Mithali To Anchor Indian Team Against Australi...   \n",
       "4       After the assent of the Honble President on 8t...   \n",
       "...                                                   ...   \n",
       "127700  Examples of art deco construction can be found...   \n",
       "127701                          and put it in our cheeks.   \n",
       "127702  As for the other derivatives of sulphur , the ...   \n",
       "127703  its complicated functioning is defined thus in...   \n",
       "127704  They've just won four government contracts to ...   \n",
       "\n",
       "                                           hindi_sentence  \n",
       "0       आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...  \n",
       "1       और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...  \n",
       "2       जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...  \n",
       "3         आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को  \n",
       "4       8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...  \n",
       "...                                                   ...  \n",
       "127700  आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...  \n",
       "127701                    और अपने गालों में डाल लेते हैं।  \n",
       "127702  जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...  \n",
       "127703  Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .  \n",
       "127704  हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...  \n",
       "\n",
       "[127705 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c5364",
   "metadata": {},
   "source": [
    "There are 127705 rows in the dataset. Use train_len rows for training and remaining for validation. I am running on cpu. Will use gpu later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a11f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:train_len,:]\n",
    "# test_df = df.iloc[train_len: train_len + 50, :]\n",
    "test_df = df.iloc[train_len: train_len + test_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9643a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>However, Paes, who was partnering Australia's ...</td>\n",
       "      <td>आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whosoever desires the reward of the world, wit...</td>\n",
       "      <td>और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The value of insects in the biosphere is enorm...</td>\n",
       "      <td>जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mithali To Anchor Indian Team Against Australi...</td>\n",
       "      <td>आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After the assent of the Honble President on 8t...</td>\n",
       "      <td>8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  However, Paes, who was partnering Australia's ...   \n",
       "1  Whosoever desires the reward of the world, wit...   \n",
       "2  The value of insects in the biosphere is enorm...   \n",
       "3  Mithali To Anchor Indian Team Against Australi...   \n",
       "4  After the assent of the Honble President on 8t...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...  \n",
       "1  और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...  \n",
       "2  जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...  \n",
       "3    आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को  \n",
       "4  8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d13d6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>and the amount of climate emissions</td>\n",
       "      <td>और उन मौसमी उत्सर्जनों के बीच</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>“ I told them , ' Never cover up for me , ' ” ...</td>\n",
       "      <td>जनरल कहते हैं , ' ' मैंने उनसे कहा , कभी मेरी ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>as much as the anger part of it.</td>\n",
       "      <td>जितना की उसके बाद का गुस्सा.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>Preventing Real Online Threats</td>\n",
       "      <td>प्रिवेंटिंग रियल ऑनलाइन थ्रेट्स</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>Besides, it is also believed that Man can not ...</td>\n",
       "      <td>साथ में यह भी माना जाता कि उसकी पूरी कल्पना मन...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence  \\\n",
       "100000                and the amount of climate emissions   \n",
       "100001  “ I told them , ' Never cover up for me , ' ” ...   \n",
       "100002                   as much as the anger part of it.   \n",
       "100003                     Preventing Real Online Threats   \n",
       "100004  Besides, it is also believed that Man can not ...   \n",
       "\n",
       "                                           hindi_sentence  \n",
       "100000                      और उन मौसमी उत्सर्जनों के बीच  \n",
       "100001  जनरल कहते हैं , ' ' मैंने उनसे कहा , कभी मेरी ...  \n",
       "100002                       जितना की उसके बाद का गुस्सा.  \n",
       "100003                    प्रिवेंटिंग रियल ऑनलाइन थ्रेट्स  \n",
       "100004  साथ में यह भी माना जाता कि उसकी पूरी कल्पना मन...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264b6aa",
   "metadata": {},
   "source": [
    "Create lists of source and target sentences for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b4c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "source_train = train_df['english_sentence'].tolist()\n",
    "target_train = train_df['hindi_sentence'].tolist()\n",
    "\n",
    "# test\n",
    "source_test = test_df['english_sentence'].tolist()\n",
    "target_test = test_df['hindi_sentence'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67791a11",
   "metadata": {},
   "source": [
    "Create training and testing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc142909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "train_dataset = TranslationDataset(src=source_train, target=target_train,tokenizer=tokenizer, src_prepend_value=src_prepend_value, target_prepend_value=target_prepend_value)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = TranslationDataset(src=source_test, target=target_test,tokenizer=tokenizer, src_prepend_value=src_prepend_value, target_prepend_value=target_prepend_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d2239d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataloader\n",
    "train_dataloader = TranslationDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test dataloader\n",
    "test_dataloader = TranslationDataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc9ad5",
   "metadata": {},
   "source": [
    "set aside a input for inference later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5b3b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(test_dataloader)\n",
    "sample_batch = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b2ec7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|english|>It lasted throughout his life .<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|hindi|>')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.decode(sample_batch[0][0])\n",
    "target = tokenizer.decode(sample_batch[1][0][:1]) # take the starting token for now.\n",
    "\n",
    "inputs, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360deec",
   "metadata": {},
   "source": [
    "Create the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29758699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (token_embeddings): Embedding(50260, 512)\n",
       "    (pos_embeddings): Embedding(3000, 512)\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (mlp): MLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_embeddings): Embedding(50260, 512)\n",
       "    (pos_embeddings): Embedding(3000, 512)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (mlp): MLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (c_attn): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (norm3): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (final_layer): Linear(in_features=512, out_features=50260, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(random_seed) # needed to get same weights for reproducibility\n",
    "model = Transformer(vocab_size=vocab_size, context_length=context_length, emb_dim=emb_dim, enc_layers=enc_layers, dec_layers=dec_layers, num_heads=num_heads,dropout=dropout, bias=bias)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb738f8",
   "metadata": {},
   "source": [
    "Create a optimizer and loss function\n",
    "\n",
    "Using Adam optimizer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e287f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5c2cf",
   "metadata": {},
   "source": [
    "### Create the trainer instance for training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745e5db",
   "metadata": {},
   "source": [
    "##### create a path to save model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b228a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_dir = home_dir / 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0401969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "541754f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The path do not exist yet.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer, tokenizer=tokenizer, save_path= model_checkpoint_dir / 'transformer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "769a3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                          | 217/6250 [00:29<13:37,  7.38it/s]\n",
      "  0%|          | 0/10 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eng-indic-transformer/venv/lib/python3.12/site-packages/en_indic_transformer/model.py:96\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataloader, test_dataloader, epochs, device, predict_input, predict_target, max_tokens)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# loop over the dataloader\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, (x, y_in, y_out) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dataloader, leave=\u001b[38;5;28;01mTrue\u001b[39;00m, ncols=\u001b[32m100\u001b[39m)):\n\u001b[32m     95\u001b[39m     \u001b[38;5;66;03m# move the input to respective device\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     x, y_in, y_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# calculate y_logits by performing feed-forward.\u001b[39;00m\n\u001b[32m     99\u001b[39m     y_logits = \u001b[38;5;28mself\u001b[39m.model(x, y_in)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eng-indic-transformer/venv/lib/python3.12/site-packages/en_indic_transformer/model.py:54\u001b[39m, in \u001b[36mTrainer.move\u001b[39m\u001b[34m(self, x, y_in, y_out, device)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmove\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     x: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     device: DeviceType = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     50\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m:\n\u001b[32m     51\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    Moves the passed tensors into their respective device\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     y_in = y_in.to(device)\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataloader=train_dataloader, test_dataloader=test_dataloader, epochs=epochs, device=device, predict_input=inputs, predict_target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46af1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
